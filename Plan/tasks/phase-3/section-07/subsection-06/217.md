# PHASE3-049: Add missing tests

**Section**: 7. Testing Review
**Subsection**: 7.6
**Task ID**: PHASE3-049

## Description

Review the codebase and add missing tests to ensure comprehensive test coverage. This task involves identifying untested code, adding unit tests, integration tests, and edge case tests to improve overall test coverage and ensure code reliability.

## Checklist

### Coverage Analysis

- [ ] Run test coverage report using `npm run test:coverage`
- [ ] Review coverage report to identify uncovered code areas
- [ ] Identify source files in `src/` directory that lack test coverage
- [ ] Review coverage gaps across different directories:
  - [ ] Controllers (`src/controllers/`)
  - [ ] Services (`src/services/`)
  - [ ] Models (`src/models/`)
  - [ ] Middleware (`src/middleware/`)
  - [ ] Utils (`src/utils/`)
  - [ ] Routes (`src/routes/`)
  - [ ] Config (`src/config/`)
  - [ ] Types (`src/types/`)
  - [ ] Errors (`src/errors/`)
  - [ ] Validators (`src/validators/`)
  - [ ] Jobs (`src/jobs/`)
- [ ] Document uncovered files and functions

### Unit Test Creation

- [ ] Add unit tests for uncovered controllers
  - [ ] Create test files in `tests/unit/controllers/`
  - [ ] Test all public methods
  - [ ] Mock external dependencies using existing mocks (`tests/mocks/`)
  - [ ] Use test fixtures from `tests/fixtures/` where appropriate
- [ ] Add unit tests for uncovered services
  - [ ] Create test files in `tests/unit/services/`
  - [ ] Test business logic and service methods
  - [ ] Mock external API calls (Telegram API, Cursor Runner API)
  - [ ] Test error handling and edge cases
- [ ] Add unit tests for uncovered models
  - [ ] Create test files in `tests/unit/models/`
  - [ ] Test model methods and validations
  - [ ] Test database interactions (with mocked database)
- [ ] Add unit tests for uncovered middleware
  - [ ] Create test files in `tests/unit/middleware/`
  - [ ] Test request/response handling
  - [ ] Test authentication and authorization logic
  - [ ] Test error handling middleware
- [ ] Add unit tests for uncovered utilities
  - [ ] Create test files in `tests/unit/utils/`
  - [ ] Test utility functions with various inputs
  - [ ] Test edge cases and error conditions
- [ ] Add unit tests for route handlers
  - [ ] Create test files in `tests/unit/routes/`
  - [ ] Test route handler functions
  - [ ] Mock Express request/response objects
- [ ] Add unit tests for uncovered error classes
  - [ ] Create test files in `tests/unit/errors/`
  - [ ] Test custom error classes and error handling
  - [ ] Test error message formatting
- [ ] Add unit tests for uncovered validators
  - [ ] Create test files in `tests/unit/validators/`
  - [ ] Test validation logic with various inputs
  - [ ] Test validation error cases
- [ ] Add unit tests for uncovered jobs
  - [ ] Create test files in `tests/unit/jobs/`
  - [ ] Test job processing logic
  - [ ] Mock queue and job dependencies
  - [ ] Test job error handling and retries

### Integration Test Creation

- [ ] Add integration tests for API endpoints
  - [ ] Create test files in `tests/integration/api/`
  - [ ] Use Supertest to test HTTP endpoints
  - [ ] Test complete request/response cycles
  - [ ] Test authentication flows
  - [ ] Test error responses
- [ ] Add integration tests for services
  - [ ] Create test files in `tests/integration/services/`
  - [ ] Test service interactions with mocked external APIs
  - [ ] Test service-to-service communication
  - [ ] Test database service interactions (with test database)

### Edge Case and Error Scenario Tests

- [ ] Add tests for edge cases:
  - [ ] Empty/null inputs
  - [ ] Boundary values
  - [ ] Invalid data formats
  - [ ] Missing required fields
  - [ ] Extremely large inputs
- [ ] Add tests for error scenarios:
  - [ ] Network failures
  - [ ] API timeouts
  - [ ] Invalid responses from external APIs
  - [ ] Database connection failures
  - [ ] Authentication failures
  - [ ] Authorization failures
- [ ] Add tests for error handling:
  - [ ] Verify appropriate error messages
  - [ ] Verify correct HTTP status codes
  - [ ] Verify error logging

### Test Quality Checks

- [ ] Ensure all new tests follow AAA pattern (Arrange, Act, Assert)
- [ ] Ensure test names are descriptive and clear
- [ ] Ensure tests are independent and can run in any order
- [ ] Ensure proper cleanup in `afterEach`/`afterAll` hooks
- [ ] Verify tests use appropriate mocks from `tests/mocks/`
- [ ] Verify tests use fixtures from `tests/fixtures/` where appropriate
- [ ] Ensure tests follow patterns in `tests/README.md`

### Verification

- [ ] Run all tests: `npm test`
- [ ] Run unit tests: `npm run test:unit`
- [ ] Run integration tests: `npm run test:integration`
- [ ] Verify all tests pass
- [ ] Run coverage report again: `npm run test:coverage`
- [ ] Verify coverage improvement
- [ ] Check that coverage meets minimum thresholds (80% for new code)

### Documentation

- [ ] Document test additions in this task file
- [ ] List newly created test files
- [ ] Document any test patterns or conventions established
- [ ] Update `tests/README.md` if new patterns are introduced
- [ ] Document any uncovered code that cannot be tested (with justification)

## Notes

- This task is part of Phase 3: Holistic Review and Best Practices
- Section: 7. Testing Review
- Subsection: 7.6 - Adding Missing Tests
- Focus on identifying untested code and adding comprehensive tests
- Document findings and test additions

- **Test Infrastructure**: The project has test infrastructure in place:
  - Jest configured in `jest.config.ts` for unit and integration tests
  - Test setup in `tests/setup.ts`
  - Mocks available in `tests/mocks/` (telegramApi, cursorRunnerApi, redis)
  - Fixtures available in `tests/fixtures/` (telegramMessages, apiResponses)
  - Test helpers in `tests/helpers/` (testUtils, apiHelpers)
  - Test directory structure mirrors `src/` structure

- **Test Naming**: Follow conventions:
  - Unit/integration tests: `*.test.ts` suffix
  - E2E tests: `*.spec.ts` suffix (Playwright)
  - Test files should mirror source file structure

- **Test Best Practices**:
  - Use AAA pattern (Arrange, Act, Assert)
  - Mock all external dependencies
  - Keep tests independent and isolated
  - Use descriptive test names
  - Test both happy paths and error cases
  - Aim for high coverage of business logic

- **Coverage Goals**:
  - Minimum 80% coverage for new code
  - 100% coverage for critical business logic
  - Focus on controllers, services, and core functionality

- **Current State**: The codebase may have minimal source code initially. Focus on:
  - Testing any existing source files in `src/`
  - Establishing test patterns for future code
  - Ensuring test infrastructure is ready for use

- Task can be completed independently by a single agent

## Related Tasks

- Previous: PHASE3-048
- Next: PHASE3-050

---

IMPORTANT: When updating system settings (SystemSetting model), you MUST use the cursor-runner-shared-sqlite MCP connection.

IMPORTANT: When working with tasks (creating, querying, updating, or deleting tasks), you MUST use the cursor-runner-shared-sqlite MCP connection. The tasks table is in the shared SQLite database at /app/shared_db/shared.sqlite3.

Tasks Table Schema:

- id: INTEGER PRIMARY KEY AUTOINCREMENT
- prompt: TEXT NOT NULL (the task prompt/description to be executed)
- status: INTEGER NOT NULL DEFAULT 0 (task status enum: 0=ready, 1=complete, 2=archived, 3=backlogged)
- createdat: DATETIME DEFAULT CURRENT_TIMESTAMP
- updatedat: DATETIME DEFAULT CURRENT_TIMESTAMP
- order: INTEGER DEFAULT 0 (lower numbers are processed first)
- uuid: TEXT (unique identifier, indexed)

Task Status Values:

- 0 = ready (ready to be processed by task operator)
- 1 = complete (task has been completed)
- 2 = archived (task has been archived)
- 3 = backlogged (task is in backlog, not ready for processing)

Task Management Examples:

- To create a ready task: INSERT INTO tasks (prompt, "order", status) VALUES ('your prompt here', 0, 0)
- To list ready tasks: SELECT \* FROM tasks WHERE status = 0 ORDER BY "order" ASC, id ASC
- To mark a task as complete: UPDATE tasks SET status = 1, updatedat = CURRENT_TIMESTAMP WHERE id = ?
- To archive a task: UPDATE tasks SET status = 2, updatedat = CURRENT_TIMESTAMP WHERE id = ?
- To backlog a task: UPDATE tasks SET status = 3, updatedat = CURRENT_TIMESTAMP WHERE id = ?
- To get next ready task: SELECT \* FROM tasks WHERE status = 0 ORDER BY "order" ASC, id ASC LIMIT 1

The task operator agent (when enabled) automatically processes tasks with status = 0 (ready), sending the prompt to cursor-runner for execution.

IMPORTANT: When working with cursor-agents (creating, listing, getting status, or deleting agents), use the Python scripts in /cursor/tools/cursor-agents/ directory. These scripts communicate with the cursor-agents service over HTTP:

Agent Management:

- To list all agents: python3 /cursor/tools/cursor-agents/list_agents.py
- To get agent status: python3 /cursor/tools/cursor-agents/get_agent_status.py --name <agent-name>
- To create an agent: python3 /cursor/tools/cursor-agents/create_agent.py --name <name> --target-url <url> [options]
  - Use --queue <queue-name> to assign the agent to a specific queue (defaults to "default" if not specified)
  - Use --schedule <cron-pattern> for recurring agents (e.g., "0 8 \* \* \*" for daily at 8 AM)
  - Use --one-time for one-time agents that run immediately
- To delete an agent: python3 /cursor/tools/cursor-agents/delete_agent.py --name <agent-name>

Queue Management:

- To list all queues: python3 /cursor/tools/cursor-agents/list_queues.py
- To get queue info: python3 /cursor/tools/cursor-agents/get_queue_info.py --queue-name <queue-name>
- To delete an empty queue: python3 /cursor/tools/cursor-agents/delete_queue.py --queue-name <queue-name>
  - Note: Cannot delete the "default" queue or queues with active jobs

Task Operator Management:

- To enable the task operator: python3 /cursor/tools/cursor-agents/enable_task_operator.py [--queue <queue-name>]
  - The task operator automatically processes tasks from the tasks table in the database
  - It checks for incomplete tasks (lowest order first) and sends them to cursor-runner
  - Automatically re-enqueues itself every 5 seconds while enabled
- To disable the task operator: python3 /cursor/tools/cursor-agents/disable_task_operator.py
  - Sets the task_operator system setting to false, stopping re-enqueueing

When creating an agent, the target URL should be the cursor-runner docker networked URL (http://cursor-runner:3001/cursor/iterate/async) with a prompt that this agent will later execute.

Queue Organization: Agents can be organized into queues to avoid queue bloat. By default, agents are created in the "default" queue. Use descriptive queue names like "daily-tasks", "hourly-sync", or "urgent-jobs" to group related agents together.

IMPORTANT: When creating one-time scripts (shell scripts, Python scripts, etc.), place them in /cursor/scripts. This directory is shared and persistent across container restarts. Do not create scripts in the repository directories or other temporary locations.
