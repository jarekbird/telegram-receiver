# PHASE4-009: Generate automated code quality report

**Section**: 1. Automated Code Smell Detection
**Subsection**: 1.9
**Task ID**: PHASE4-009

## Description

Compile all automated code quality findings from Phase 4 Section 1 tasks (PHASE4-001 through PHASE4-008) into a comprehensive, actionable code quality report. This report should consolidate ESLint findings, complexity metrics, code duplication, unused code, code smells, and dependency analysis into a single document with metrics, scores, prioritized issues, and actionable recommendations to improve code quality and maintainability.

## Current State

The project has completed (or should have completed) the following automated code quality detection tasks:

- **PHASE4-001**: ESLint configuration with TypeScript rules - provides linting findings
- **PHASE4-002**: Prettier configuration - provides formatting consistency
- **PHASE4-003**: SonarQube or similar code quality tool setup - provides comprehensive code quality metrics
- **PHASE4-004**: Complexity analysis - provides cyclomatic complexity metrics
- **PHASE4-005**: Code duplication detection - provides duplication percentage and duplicate code blocks
- **PHASE4-006**: Unused code/dead code detection - provides unused exports, files, and dependencies
- **PHASE4-007**: Code smell detection - provides long methods, large classes, deep nesting, magic numbers
- **PHASE4-008**: Dependency analysis - provides circular dependencies, unused dependencies, security vulnerabilities, outdated packages

**Missing**: A consolidated code quality report that:

- Compiles all findings from the above tasks into a single document
- Provides overall code quality metrics and scores
- Prioritizes issues by severity and impact
- Includes actionable recommendations
- Creates a summary dashboard for quick reference
- Documents the report format for future updates

## Checklist

### Data Collection

- [ ] Gather ESLint findings from PHASE4-001 (warnings, errors, rule violations)
- [ ] Collect Prettier formatting status from PHASE4-002
- [ ] Compile SonarQube/code quality tool metrics from PHASE4-003 (if available)
- [ ] Collect complexity analysis results from PHASE4-004 (cyclomatic complexity, file complexity)
- [ ] Gather code duplication findings from PHASE4-005 (duplication percentage, duplicate blocks)
- [ ] Collect unused code findings from PHASE4-006 (unused exports, files, dependencies)
- [ ] Gather code smell findings from PHASE4-007 (long methods, large classes, deep nesting, magic numbers)
- [ ] Collect dependency analysis results from PHASE4-008 (circular dependencies, unused deps, security issues, outdated packages)
- [ ] Run current test coverage analysis (`npm run test:coverage`)
- [ ] Collect TypeScript type-checking results (`npm run type-check`)

### Report Structure

- [ ] Create executive summary section with overall code quality score
- [ ] Create metrics overview section with key numbers:
  - Total files analyzed
  - Total lines of code
  - Test coverage percentage
  - Code duplication percentage
  - Number of code smells found
  - Number of security vulnerabilities
  - Number of outdated dependencies
- [ ] Create prioritized issues section organized by:
  - Critical issues (security vulnerabilities, breaking issues)
  - High priority (code smells affecting maintainability)
  - Medium priority (code quality improvements)
  - Low priority (nice-to-have improvements)
- [ ] Create detailed findings sections for each analysis type:
  - ESLint findings with rule violations
  - Complexity metrics with files exceeding thresholds
  - Code duplication with specific duplicate blocks
  - Unused code with specific unused exports/files
  - Code smells with specific locations and recommendations
  - Dependency issues with specific packages and recommendations
- [ ] Create recommendations section with actionable next steps
- [ ] Create summary dashboard with visual metrics (using markdown tables/formatting)

### Metrics and Scoring

- [ ] Calculate overall code quality score (0-100 scale) based on:
  - Test coverage (weight: 20%)
  - Code duplication percentage (weight: 15%)
  - Code smell count (weight: 20%)
  - Security vulnerabilities (weight: 25%)
  - ESLint errors/warnings (weight: 10%)
  - Dependency health (weight: 10%)
- [ ] Create quality grade (A-F) based on overall score:
  - A: 90-100 (Excellent)
  - B: 80-89 (Good)
  - C: 70-79 (Fair)
  - D: 60-69 (Needs Improvement)
  - F: <60 (Poor)
- [ ] Include trend indicators (if historical data available)
- [ ] Calculate technical debt estimate (hours/days to fix all issues)

### Prioritization

- [ ] Categorize all issues by severity:
  - Critical: Security vulnerabilities, breaking bugs, critical code smells
  - High: Major code smells, high complexity, unused dependencies
  - Medium: Code quality improvements, moderate complexity
  - Low: Minor improvements, style issues
- [ ] Prioritize issues by impact:
  - Issues affecting multiple files/modules
  - Issues affecting core functionality
  - Issues affecting maintainability
  - Issues affecting performance
- [ ] Create prioritized action plan with estimated effort for each category

### Documentation

- [ ] Save comprehensive report to `docs/code-quality-report.md`
- [ ] Create summary dashboard section at the top of the report
- [ ] Document report format and structure for future updates
- [ ] Include instructions on how to regenerate the report
- [ ] Add report generation date and version
- [ ] Include links to related documentation and tools
- [ ] Document how to interpret metrics and scores

### Integration

- [ ] Add script to package.json to generate report: `npm run quality:report`
- [ ] Document how to run individual analysis tools
- [ ] Create template for future report updates
- [ ] Consider automating report generation in CI/CD (optional)

## Report Format Example

The code quality report should follow this structure:

```markdown
# Code Quality Report

**Generated**: [Date]
**Version**: [Version]
**Overall Score**: [Score]/100 (Grade: [A-F])

## Executive Summary

[Brief overview of code quality status, key metrics, and top priorities]

## Summary Dashboard

| Metric                | Value  | Status    |
| --------------------- | ------ | --------- |
| Overall Score         | 85/100 | B         |
| Test Coverage         | 78%    | Good      |
| Code Duplication      | 5.2%   | Excellent |
| Code Smells           | 12     | Good      |
| Security Issues       | 0      | Excellent |
| ESLint Errors         | 3      | Good      |
| Outdated Dependencies | 5      | Fair      |

## Prioritized Issues

### Critical (0 issues)

[None or list critical issues]

### High Priority (3 issues)

1. [Issue description with file location and recommendation]
2. [Issue description with file location and recommendation]
3. [Issue description with file location and recommendation]

### Medium Priority (5 issues)

[...]

### Low Priority (4 issues)

[...]

## Detailed Findings

### ESLint Findings

[Detailed ESLint violations with file locations]

### Complexity Analysis

[Files with high complexity metrics]

### Code Duplication

[Duplicate code blocks with locations]

### Unused Code

[Unused exports, files, dependencies]

### Code Smells

[Long methods, large classes, deep nesting, magic numbers]

### Dependency Analysis

[Circular dependencies, unused dependencies, security vulnerabilities, outdated packages]

## Recommendations

1. [Actionable recommendation with priority and estimated effort]
2. [Actionable recommendation with priority and estimated effort]
   ...

## Technical Debt Estimate

- Critical issues: [X] hours
- High priority: [X] hours
- Medium priority: [X] hours
- Low priority: [X] hours
- **Total**: [X] hours

## Next Steps

[Prioritized action plan for addressing issues]
```

## Notes

- This task is part of Phase 4: Code Quality Audit
- Section: 1. Automated Code Smell Detection
- This task consolidates findings from PHASE4-001 through PHASE4-008
- The report should be actionable and prioritize issues by severity and impact
- Focus on providing clear metrics, scores, and recommendations
- The report format should be consistent and easy to update in the future
- Consider creating a script to automate report generation from individual tool outputs
- The report should serve as a baseline for tracking code quality improvements over time
- Test coverage data should be collected from Jest coverage reports
- TypeScript type-checking results should be collected from `tsc --noEmit`
- All findings should include file paths and line numbers where applicable
- Prioritization should consider both severity and business impact
- Technical debt estimates help stakeholders understand the effort required to improve code quality

- Task can be completed independently by a single agent

## Related Tasks

- Previous: PHASE4-008
- Next: PHASE4-010

## Definition of Done

This document defines the criteria for task completion. The review agent uses these definitions to evaluate whether a task has been completed successfully.

### 1. CODE/FILE WRITING TASKS

**Description**: Tasks that involve writing, creating, modifying, or implementing SOURCE CODE FILES that need to be committed to git.

**Definition of Done**: "A Pull Request was created OR code was pushed to origin with the task complete"

**Examples**:
- Creating new source code files
- Modifying existing source code files
- Implementing features, functions, classes, modules
- Writing tests, specs
- Refactoring code
- Fixing bugs in source code

### 2. SYSTEM/ENVIRONMENT OPERATION TASKS

**Description**: Tasks involving installing dependencies, running builds, installing packages, running migrations, executing install scripts, etc.

**Definition of Done**: "The required operation must complete successfully with no errors, and the expected artifacts must be created. If any part of the operation fails, the task is NOT complete."

**Important Notes**:
- Installing dependencies requires packages to actually be installed successfully
- Updating package.json is NOT enough
- If the output mentions environmental issues, errors, warnings, or failed operations, the task is NOT complete

**Examples**:
- Installing npm packages, pip packages, gem dependencies
- Running database migrations
- Building/compiling projects
- Setting up development environments
- Running install scripts

### 3. SIMPLE REQUESTS/QUESTIONS/DATA OPERATIONS

**Description**: Tasks involving asking questions, requesting information, explanations, clarifications, database queries/updates, data manipulation, executing scripts/commands that don't create source code files, etc.

**Definition of Done**: "The request was completed or the question was answered"

**Examples**:
- Answering questions
- Providing explanations
- Querying databases
- Reading/updating data
- Executing commands that don't create source code files
- Information gathering tasks



---

IMPORTANT: When updating system settings (SystemSetting model), you MUST use the cursor-runner-shared-sqlite MCP connection.

IMPORTANT: When working with tasks (creating, querying, updating, or deleting tasks), you MUST use the cursor-runner-shared-sqlite MCP connection. The tasks table is in the shared SQLite database at /app/shared_db/shared.sqlite3.

Tasks Table Schema:

- id: INTEGER PRIMARY KEY AUTOINCREMENT
- prompt: TEXT NOT NULL (the task prompt/description to be executed)
- status: INTEGER NOT NULL DEFAULT 0 (task status enum: 0=ready, 1=complete, 2=archived, 3=backlogged)
- createdat: DATETIME DEFAULT CURRENT_TIMESTAMP
- updatedat: DATETIME DEFAULT CURRENT_TIMESTAMP
- order: INTEGER DEFAULT 0 (lower numbers are processed first)
- uuid: TEXT (unique identifier, indexed)

Task Status Values:

- 0 = ready (ready to be processed by task operator)
- 1 = complete (task has been completed)
- 2 = archived (task has been archived)
- 3 = backlogged (task is in backlog, not ready for processing)

Task Management Examples:

- To create a ready task: INSERT INTO tasks (prompt, "order", status) VALUES ('your prompt here', 0, 0)
- To list ready tasks: SELECT \* FROM tasks WHERE status = 0 ORDER BY "order" ASC, id ASC
- To mark a task as complete: UPDATE tasks SET status = 1, updatedat = CURRENT_TIMESTAMP WHERE id = ?
- To archive a task: UPDATE tasks SET status = 2, updatedat = CURRENT_TIMESTAMP WHERE id = ?
- To backlog a task: UPDATE tasks SET status = 3, updatedat = CURRENT_TIMESTAMP WHERE id = ?
- To get next ready task: SELECT \* FROM tasks WHERE status = 0 ORDER BY "order" ASC, id ASC LIMIT 1

The task operator agent (when enabled) automatically processes tasks with status = 0 (ready), sending the prompt to cursor-runner for execution.

IMPORTANT: When working with cursor-agents (creating, listing, getting status, or deleting agents), use the Python scripts in /cursor/tools/cursor-agents/ directory. These scripts communicate with the cursor-agents service over HTTP:

Agent Management:

- To list all agents: python3 /cursor/tools/cursor-agents/list_agents.py
- To get agent status: python3 /cursor/tools/cursor-agents/get_agent_status.py --name <agent-name>
- To create an agent: python3 /cursor/tools/cursor-agents/create_agent.py --name <name> --target-url <url> [options]
  - Use --queue <queue-name> to assign the agent to a specific queue (defaults to "default" if not specified)
  - Use --schedule <cron-pattern> for recurring agents (e.g., "0 8 \* \* \*" for daily at 8 AM)
  - Use --one-time for one-time agents that run immediately
- To delete an agent: python3 /cursor/tools/cursor-agents/delete_agent.py --name <agent-name>

Queue Management:

- To list all queues: python3 /cursor/tools/cursor-agents/list_queues.py
- To get queue info: python3 /cursor/tools/cursor-agents/get_queue_info.py --queue-name <queue-name>
- To delete an empty queue: python3 /cursor/tools/cursor-agents/delete_queue.py --queue-name <queue-name>
  - Note: Cannot delete the "default" queue or queues with active jobs

Task Operator Management:

- To enable the task operator: python3 /cursor/tools/cursor-agents/enable_task_operator.py [--queue <queue-name>]
  - The task operator automatically processes tasks from the tasks table in the database
  - It checks for incomplete tasks (lowest order first) and sends them to cursor-runner
  - Automatically re-enqueues itself every 5 seconds while enabled
- To disable the task operator: python3 /cursor/tools/cursor-agents/disable_task_operator.py
  - Sets the task_operator system setting to false, stopping re-enqueueing

When creating an agent, the target URL should be the cursor-runner docker networked URL (http://cursor-runner:3001/cursor/iterate/async) with a prompt that this agent will later execute.

Queue Organization: Agents can be organized into queues to avoid queue bloat. By default, agents are created in the "default" queue. Use descriptive queue names like "daily-tasks", "hourly-sync", or "urgent-jobs" to group related agents together.

IMPORTANT: When creating one-time scripts (shell scripts, Python scripts, etc.), place them in /cursor/scripts. This directory is shared and persistent across container restarts. Do not create scripts in the repository directories or other temporary locations.
