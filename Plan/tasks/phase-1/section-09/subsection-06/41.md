# PHASE1-041: Test docker-compose.prod.yml

**Section**: 9. Docker Configuration
**Subsection**: 9.6
**Task ID**: PHASE1-041

## Description

Test the docker-compose.prod.yml configuration for the telegram-receiver Node.js/TypeScript application. This task verifies that all services defined in docker-compose.prod.yml (created in PHASE1-039) start correctly, communicate properly, and function as expected. This includes testing Traefik reverse proxy, Redis service, application service, worker service, shared volumes, network connectivity, health checks, and proper service shutdown. This ensures the docker-compose configuration is correct and ready for production deployment.

**Note**: The curl commands in this task are for Docker deployment verification only. For development testing, use automated tests (`npm test`, `npm run test:integration`) instead of running the server manually.

## Prerequisites

- PHASE1-039 (Create docker-compose.prod.yml) must be completed
- PHASE1-040 (Test Docker build) must be completed
- docker-compose.prod.yml must exist in the project root
- Dockerfile must exist and build successfully
- `.dockerignore` file should exist (from PHASE1-037)
- `virtual-assistant-network` Docker network must exist (create with `docker network create virtual-assistant-network` if needed)

## Checklist

- [ ] Verify `docker-compose.prod.yml` exists in project root
- [ ] Verify `Dockerfile` exists (from PHASE1-036)
- [ ] Verify `.dockerignore` file exists (from PHASE1-037)
- [ ] Verify `virtual-assistant-network` exists: `docker network ls | grep virtual-assistant-network`
  - [ ] If missing, create it: `docker network create virtual-assistant-network`
- [ ] Verify worker script/entry point exists (required for worker service):
  - [ ] Check if `package.json` has `"worker"` script defined, OR
  - [ ] Verify worker entry point exists (e.g., `dist/worker.js` or `src/worker.ts`)
  - [ ] Note: If worker script doesn't exist, it must be created before testing (see PHASE1-039 notes)
- [ ] Set up required environment variables (create `.env` file or export):
  - [ ] `DOMAIN_NAME` (e.g., `localhost` for testing)
  - [ ] `ACME_EMAIL` (for Let's Encrypt, e.g., `admin@example.com`)
  - [ ] `TELEGRAM_BOT_TOKEN` (if needed for app startup)
  - [ ] `CURSOR_RUNNER_TIMEOUT` (optional, defaults to 300)
- [ ] Run `docker-compose -f docker-compose.prod.yml up --build -d` (detached mode)
- [ ] Wait for all services to start (check logs: `docker-compose -f docker-compose.prod.yml logs`)
- [ ] Verify all containers are running: `docker-compose -f docker-compose.prod.yml ps`
  - [ ] `telegram-receiver-traefik` container should be running
  - [ ] `telegram-receiver-redis` container should be running
  - [ ] `telegram-receiver-app` container should be running
  - [ ] `telegram-receiver-worker` container should be running
- [ ] Check Traefik service:
  - [ ] Verify Traefik container logs show no errors: `docker logs telegram-receiver-traefik`
  - [ ] Verify Traefik dashboard is accessible (if enabled): `curl http://localhost:8080` (optional)
- [ ] Check Redis service:
  - [ ] Verify Redis container logs show no errors: `docker logs telegram-receiver-redis`
  - [ ] Test Redis connectivity: `docker exec telegram-receiver-redis redis-cli ping` (should return `PONG`)
  - [ ] Verify Redis healthcheck passed: `docker inspect telegram-receiver-redis | grep -A 5 Health`
- [ ] Check application service:
  - [ ] Verify app container logs show no errors: `docker logs telegram-receiver-app`
  - [ ] Verify app container logs show application started successfully
  - [ ] Test health endpoint via Traefik: `curl -k https://localhost/health` (or `curl http://localhost/health` if HTTP redirect works)
  - [ ] Test health endpoint directly (if port exposed for testing): `curl http://localhost:3000/health` (should return 200 OK)
  - [ ] Verify health endpoint response contains expected JSON structure (matching Rails implementation):
    - [ ] `status: "healthy"` (must be exactly "healthy")
    - [ ] `service: "telegram-receiver"` (service name)
    - [ ] `version: "..."` (version number, e.g., "1.0.0")
  - [ ] Verify app healthcheck passed: `docker inspect telegram-receiver-app | grep -A 5 Health`
- [ ] Check worker service:
  - [ ] Verify worker container logs show no errors: `docker logs telegram-receiver-worker`
  - [ ] Verify worker container logs show worker started successfully
  - [ ] Verify worker can connect to Redis (check logs for connection success)
  - [ ] Verify worker process is running: `docker exec telegram-receiver-worker ps aux | grep -E "(node|worker)"`
- [ ] Verify network connectivity:
  - [ ] Verify all containers are on `virtual-assistant-network`: `docker network inspect virtual-assistant-network`
  - [ ] Test app can reach Redis: `docker exec telegram-receiver-app ping -c 1 redis` (or test Redis connection from app)
  - [ ] Test app can reach cursor-runner (if available):
    - [ ] Verify `CURSOR_RUNNER_URL` environment variable is set correctly: `docker exec telegram-receiver-app env | grep CURSOR_RUNNER_URL` (should show `CURSOR_RUNNER_URL=http://cursor-runner:3001`)
    - [ ] (Optional) Test connectivity: `docker exec telegram-receiver-app ping -c 1 cursor-runner` (if cursor-runner service is running)
- [ ] Verify volumes:
  - [ ] Verify `shared_redis_data` volume exists: `docker volume ls | grep shared_redis_data`
  - [ ] Verify `shared_sqlite_db` volume exists: `docker volume ls | grep shared_sqlite_db`
  - [ ] Verify volumes are mounted correctly in containers: `docker inspect telegram-receiver-redis | grep -A 10 Mounts`
- [ ] Test service dependencies:
  - [ ] Verify app service waits for Redis (check startup logs for connection retries if Redis was slow)
  - [ ] Verify worker service waits for Redis and app (check startup order)
- [ ] Test graceful shutdown:
  - [ ] Run `docker-compose -f docker-compose.prod.yml stop` (graceful stop)
  - [ ] Verify all containers stop gracefully: `docker-compose -f docker-compose.prod.yml ps` (should show exited)
  - [ ] Run `docker-compose -f docker-compose.prod.yml down` (remove containers)
  - [ ] Verify containers are removed: `docker ps -a | grep telegram-receiver` (should show no containers)
  - [ ] (Optional) Remove volumes: `docker-compose -f docker-compose.prod.yml down -v` (if testing cleanup)
  - [ ] (Optional) Remove images: `docker-compose -f docker-compose.prod.yml down --rmi all` (if testing full cleanup)

## Notes

- This task is part of Phase 1: Basic Node.js API Infrastructure
- Section: 9. Docker Configuration
- Task can be completed independently by a single agent
- **Prerequisites**: This task requires PHASE1-039 (Create docker-compose.prod.yml) and PHASE1-040 (Test Docker build) to be completed first
- Use `docker-compose -f docker-compose.prod.yml` to specify the production compose file
- The `virtual-assistant-network` must exist before starting services (create with `docker network create virtual-assistant-network` if needed)
- For local testing, you may need to:
  - Set `DOMAIN_NAME=localhost` in environment
  - Use self-signed certificates or disable SSL verification for testing (`curl -k`)
  - Access services directly via exposed ports if Traefik routing doesn't work in local environment
- Health checks may take time to pass - wait at least 30 seconds after starting services before checking health status
- If services fail to start, check:
  - Network exists and is accessible
  - Environment variables are set correctly
  - Ports are not already in use (especially 80, 443, 3000, 6379, 8080)
  - Docker has sufficient resources (memory, disk space)
  - Dockerfile builds successfully (tested in PHASE1-040)
- Traefik may require domain name configuration for SSL - for local testing, you may need to adjust Traefik labels or use HTTP only
- Redis must be accessible from both app and worker services
- Shared volumes enable data persistence across container restarts
- After testing, clean up containers, networks, and optionally volumes to avoid cluttering Docker
- The worker service should start successfully even if no jobs are queued
- All services should have proper restart policies (`unless-stopped`) for production reliability
- **Worker Script Requirement**: The worker service requires a worker script/entry point to exist. This may be created in a separate task (as noted in PHASE1-039). Before testing, verify that either:
  - `package.json` has a `"worker"` script (e.g., `"worker": "node dist/worker.js"`), OR
  - A worker entry point file exists (e.g., `dist/worker.js` or `src/worker.ts`)
- Health endpoint JSON structure must match the Rails implementation format: `{"status": "healthy", "service": "telegram-receiver", "version": "1.0.0"}` (see PHASE1-040 for reference)

## Related Tasks

- Previous: PHASE1-040
- Next: PHASE1-042


---

IMPORTANT: When updating system settings (SystemSetting model), you MUST use the cursor-runner-shared-sqlite MCP connection.

IMPORTANT: When working with tasks (creating, querying, updating, or deleting tasks), you MUST use the cursor-runner-shared-sqlite MCP connection. The tasks table is in the shared SQLite database at /app/shared_db/shared.sqlite3.

Tasks Table Schema:
- id: INTEGER PRIMARY KEY AUTOINCREMENT
- prompt: TEXT NOT NULL (the task prompt/description to be executed)
- status: INTEGER NOT NULL DEFAULT 0 (task status enum: 0=ready, 1=complete, 2=archived, 3=backlogged)
- createdat: DATETIME DEFAULT CURRENT_TIMESTAMP
- updatedat: DATETIME DEFAULT CURRENT_TIMESTAMP
- order: INTEGER DEFAULT 0 (lower numbers are processed first)
- uuid: TEXT (unique identifier, indexed)

Task Status Values:
- 0 = ready (ready to be processed by task operator)
- 1 = complete (task has been completed)
- 2 = archived (task has been archived)
- 3 = backlogged (task is in backlog, not ready for processing)

Task Management Examples:
- To create a ready task: INSERT INTO tasks (prompt, "order", status) VALUES ('your prompt here', 0, 0)
- To list ready tasks: SELECT * FROM tasks WHERE status = 0 ORDER BY "order" ASC, id ASC
- To mark a task as complete: UPDATE tasks SET status = 1, updatedat = CURRENT_TIMESTAMP WHERE id = ?
- To archive a task: UPDATE tasks SET status = 2, updatedat = CURRENT_TIMESTAMP WHERE id = ?
- To backlog a task: UPDATE tasks SET status = 3, updatedat = CURRENT_TIMESTAMP WHERE id = ?
- To get next ready task: SELECT * FROM tasks WHERE status = 0 ORDER BY "order" ASC, id ASC LIMIT 1

The task operator agent (when enabled) automatically processes tasks with status = 0 (ready), sending the prompt to cursor-runner for execution.

IMPORTANT: When working with cursor-agents (creating, listing, getting status, or deleting agents), use the Python scripts in /cursor/tools/cursor-agents/ directory. These scripts communicate with the cursor-agents service over HTTP:

Agent Management:
- To list all agents: python3 /cursor/tools/cursor-agents/list_agents.py
- To get agent status: python3 /cursor/tools/cursor-agents/get_agent_status.py --name <agent-name>
- To create an agent: python3 /cursor/tools/cursor-agents/create_agent.py --name <name> --target-url <url> [options]
  - Use --queue <queue-name> to assign the agent to a specific queue (defaults to "default" if not specified)
  - Use --schedule <cron-pattern> for recurring agents (e.g., "0 8 * * *" for daily at 8 AM)
  - Use --one-time for one-time agents that run immediately
- To delete an agent: python3 /cursor/tools/cursor-agents/delete_agent.py --name <agent-name>

Queue Management:
- To list all queues: python3 /cursor/tools/cursor-agents/list_queues.py
- To get queue info: python3 /cursor/tools/cursor-agents/get_queue_info.py --queue-name <queue-name>
- To delete an empty queue: python3 /cursor/tools/cursor-agents/delete_queue.py --queue-name <queue-name>
  - Note: Cannot delete the "default" queue or queues with active jobs

Task Operator Management:
- To enable the task operator: python3 /cursor/tools/cursor-agents/enable_task_operator.py [--queue <queue-name>]
  - The task operator automatically processes tasks from the tasks table in the database
  - It checks for incomplete tasks (lowest order first) and sends them to cursor-runner
  - Automatically re-enqueues itself every 5 seconds while enabled
- To disable the task operator: python3 /cursor/tools/cursor-agents/disable_task_operator.py
  - Sets the task_operator system setting to false, stopping re-enqueueing

When creating an agent, the target URL should be the cursor-runner docker networked URL (http://cursor-runner:3001/cursor/iterate/async) with a prompt that this agent will later execute.

Queue Organization: Agents can be organized into queues to avoid queue bloat. By default, agents are created in the "default" queue. Use descriptive queue names like "daily-tasks", "hourly-sync", or "urgent-jobs" to group related agents together.

IMPORTANT: When creating one-time scripts (shell scripts, Python scripts, etc.), place them in /cursor/scripts. This directory is shared and persistent across container restarts. Do not create scripts in the repository directories or other temporary locations.
